{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm1-Dgw0-GQy"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/LLM_Finetuning_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{llm-finetune-hf} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8SJSrRb-GQz"
      },
      "source": [
        "# LLM Finetuning with HuggingFace and Weights and Biases\n",
        "<!--- @wandbcode{llm-finetune-hf} -->\n",
        "- Fine-tune a lightweight LLM (OPT-125M) with LoRA and 8-bit quantization using Launch\n",
        "- Checkpoint the LoRA adapter weights as artifacts\n",
        "- Link the best checkpoint in Model Registry\n",
        "- Run inference on a quantized model\n",
        "\n",
        "The same workflow and principles from this notebook can be applied to fine-tuning some of the stronger OSS LLMs (e.g. Llama2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S14ZgMX4-GQz"
      },
      "source": [
        "### Fine-tune large models using ðŸ¤— `peft` adapters, `transformers` & `bitsandbytes`\n",
        "\n",
        "In this tutorial we will cover how we can fine-tune large language models using the very recent `peft` library and `bitsandbytes` for loading large models in 8-bit.\n",
        "The fine-tuning method will rely on a recent method called \"Low Rank Adapters\" (LoRA), instead of fine-tuning the entire model you just have to fine-tune these adapters and load them properly inside the model.\n",
        "After fine-tuning the model you can also share your adapters on the ðŸ¤— Hub and load them very easily. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yJzJREN-GQz"
      },
      "source": [
        "### Install requirements\n",
        "\n",
        "First, run the cells below to install the requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYxB79qA-GQz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
        "!pip install -q wandb\n",
        "!pip install -q ctranslate2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHYwS9QsACsq"
      },
      "source": [
        "### W&B Set-up ðŸš€\n",
        "\n",
        "### Client Configuration\n",
        "\n",
        "We need to connect the client to an account in the web server. We do this in a notebook by calling `wandb.login`.\n",
        "\n",
        "Otherwise, you will see a authoriztion link and be asked to enter an API key. If you already have an account, you can follow the authorization link and then copy and paste the displayed API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdR65TRyA0NA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Note that https://api.wandb.ai is the default and points to the publicly hosted\n",
        "# app. You'll want to change this to a different API endpoint if you are trying\n",
        "# to connect to a privately hosted server.\n",
        "#\n",
        "# You can configure this with environment variables:do\n",
        "# export WANDB_API_KEY=<your-api-key>\n",
        "# export WANDB_BASE_URL=<your-base-url>\n",
        "\n",
        "# Or in Colab\n",
        "# %env WANDB_BASE_URL=<your-base-url>\n",
        "# wandb.login(host=\"<your-base-url>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txrn1BMnBEhE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN-lEuzFA--V"
      },
      "source": [
        "### Track your experiments\n",
        "\n",
        "### `wandb.init`\n",
        "\n",
        "The `wandb.init` function initializes a new `Run`, which you can think of as a comprehensive record of your machine learning experiment. Tracking starts when you call `wandb.init` and ends when you call `wandb.finish` (called automatically via `atexit` hooks if you don't want to invoke manually). You can also use python's `with` statement to initialize and finish runs (see code cell below).\n",
        "\n",
        "#### init pattern 1\n",
        "`wandb.init()`\n",
        "\n",
        "`// code here`\n",
        "\n",
        "`wandb.finish()` # if in a notebook, otherwise not needed\n",
        "\n",
        "### init pattern 2\n",
        "`with wandb.init():`\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`// code here`\n",
        "\n",
        "### `wandb.init` arguments\n",
        "`wandb.init()` accepts a number of arguments. The full list can be found [here](https://docs.wandb.ai/ref/python/init) but the most common ones are\n",
        "- `entity` (str) which logs the run to a specific entity (individual or team)\n",
        "- `project` (str) which logs the run to a specific project\n",
        "- `config` (dict) which sets wandb.config for saving inputs to your run such as hyperparameters for a model or settings for a data preprocessing job\n",
        "\n",
        "### `wandb.log`\n",
        "\n",
        "You can call `wandb.log` within your experiment add metrics to your `Run`. The idea is that you will call `wandb.log` many times over an experiment for the same metric, in which case the run saves the whole history of each metric across all of your `wandb.log` calls. The code cell below demonstrates how this looks in a typical stochastic gradient descent loop.\n",
        "\n",
        "### Entity\n",
        "An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. Teams you are part of may appear in brackets after your username. _If you want to log to your personal entity, it is only the username that you need._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1JsKn6nASiJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "PROJECT = \"<>\" # give your project a name!\n",
        "ENTITY = \"<>\" #  for the training you can find your username here https://wandb.ai/settings or <your-base-url>/settings -- the entity can also be a W&B team that you are part of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePT0HITWAXp4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "config = dict(\n",
        "  batch_size=32,\n",
        "  learning_rate=1e-4,\n",
        "  llm='gpt-5',\n",
        "  finetune_techinque = 'DPO',\n",
        ")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The pattern of \"with wandb.init()...\" causes wandb.finish() to be called as\n",
        "soon as we leave the with block. This is especially useful when you have a script\n",
        "or notebook that initializes multiple runs that you want to track separately.\n",
        "\"\"\"\n",
        "with wandb.init(entity=ENTITY, project=PROJECT, config=config, job_type='finetune') as run:\n",
        "\n",
        "    for key, value in dict(wandb.config).items():\n",
        "        print(key, value)\n",
        "\n",
        "    # Imagine we run 100 epochs of model training / fine tuning\n",
        "    for x in range(2, 100):\n",
        "\n",
        "        # Insert model training here...\n",
        "        # ...\n",
        "\n",
        "        # Compute metrics (or in this case, make them up)\n",
        "        metrics = dict(\n",
        "            loss=(1/x)**0.25,\n",
        "            accuracy=1-(1/x)*2\n",
        "        )\n",
        "\n",
        "        # Pass metrics to Weights & Biases\n",
        "        run.log(metrics)\n",
        "    print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd12Mu4n-GQ0"
      },
      "source": [
        "### Model Loading\n",
        "\n",
        "- Here we leverage 8-bit quantization to reduce the memory footprint of the model during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC43kFER-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-125m\",\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvYdKfTL-GQ0"
      },
      "source": [
        "### Post-processing on the model\n",
        "\n",
        "Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in `float32` for stability. We also cast the output of the last layer in `float32` for the same reasons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd2Hm6jj-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u9fuRnV-GQ0"
      },
      "source": [
        "### Apply LoRA\n",
        "\n",
        "Here comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaw3iLZL-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMvoVn0X-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWmwu2af-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeP8GdA4-GQ0"
      },
      "source": [
        "### Training\n",
        "- [W&B HuggingFace integration](https://docs.wandb.ai/guides/integrations/huggingface) automatically tracks important metrics during the course of training\n",
        "- Also track the HF checkpoints as artifacts and register them in the model registry!\n",
        "- Change the number of steps to 200+ for real results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxQbeOnI-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "import wandb\n",
        "\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "\n",
        "wandb.init(project=PROJECT,\n",
        "           entity=ENTITY,\n",
        "           job_type=\"training\")\n",
        "\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        report_to=\"wandb\",\n",
        "        warmup_steps=5,\n",
        "        max_steps=25,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        save_steps=5,\n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs0juPBnUfjt"
      },
      "source": [
        "## Sweep over hyperparameters ðŸ§¹\n",
        "\n",
        "a Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evalutes them. Whether that strategy is as simple as trying every option or as complex as BOHB, Weights & Biases Sweeps have you covered. You just need to define your strategy in the form of a configuration.\n",
        "\n",
        "When you're setting up a Sweep in a notebook like this, that config object is a nested dictionary. When you run a Sweep via the command line, the config object is a YAML file.\n",
        "\n",
        "Let's walk through the definition of a Sweep config together. We'll do it slowly, so we get a chance to explain each component. In a typical Sweep pipeline, this step would be done in a single assignment.\n",
        "\n",
        "The first thing we need to define is the method for choosing new parameter values.\n",
        "\n",
        "We provide the following search methods:\n",
        "\n",
        "grid Search â€“ Iterate over every combination of hyperparameter values.\n",
        "Very effective, but can be computationally costly.\n",
        "\n",
        "random Search â€“ Select each new combination at random according to provided distributions. Surprisingly effective!\n",
        "\n",
        "bayesian Search â€“ Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\n",
        "\n",
        "We'll stick with bayesian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSj8D8-hT28e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes'\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFkKnECeUlD8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "metric = {\n",
        "    'name': 'train/loss',\n",
        "    'goal': 'minimize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCWaEKGsUlbm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "parameters_dict = {\n",
        "    'warmup_steps': {\n",
        "        'values': [5,10]\n",
        "        },\n",
        "    'max_steps': {\n",
        "        'values': [10, 15, 20]\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        'distribution': 'uniform', # https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#distribution\n",
        "        'min': 2e-4,\n",
        "        'max': 2e-2,\n",
        "      },\n",
        "      'per_device_train_batch_size': {\n",
        "        'values': [4,6,8,10,20]\n",
        "      }\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMABMPQcUt_i",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def train(config=None):\n",
        "  wandb.init()\n",
        "\n",
        "  config = wandb.config\n",
        "  for key, value in dict(config).items():\n",
        "        print(key, value)\n",
        "\n",
        "  data = load_dataset(\"Abirate/english_quotes\")\n",
        "  data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
        "\n",
        "  trainer = transformers.Trainer(\n",
        "      model=model,\n",
        "      train_dataset=data['train'],\n",
        "      args=transformers.TrainingArguments(\n",
        "          per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "          gradient_accumulation_steps=4,\n",
        "          report_to=\"wandb\",\n",
        "          warmup_steps=config.warmup_steps,\n",
        "          max_steps=config.max_steps,\n",
        "          learning_rate=config.learning_rate,\n",
        "          fp16=True,\n",
        "          logging_steps=1,\n",
        "          save_steps=5,\n",
        "          output_dir='outputs'\n",
        "      ),\n",
        "      data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "  )\n",
        "  model.config.use_cache = False\n",
        "  trainer.train()\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FteNto7XUq04",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=ENTITY, project=PROJECT) # creates sweep controller on W&B server side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nohn6A6UUwBw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count = 3)\n",
        "wandb.teardown() # if we want to do normal runs after a sweep, in the same session, we must run this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYwSyQBTVZ3r",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Find run with best loss from sweep\n",
        "\n",
        "# Initialize the wandb API\n",
        "api = wandb.Api()\n",
        "\n",
        "# Fetch the sweep\n",
        "sweep = api.sweep(f\"{ENTITY}/{PROJECT}/{sweep_id}\")\n",
        "\n",
        "# Get all runs from the sweep\n",
        "runs = sweep.runs\n",
        "\n",
        "# Initialize the best run and the highest accuracy\n",
        "best_run = None\n",
        "lowest_loss = float('inf')\n",
        "\n",
        "# Iterate over all runs to find the one with the highest accuracy\n",
        "for run in runs:\n",
        "    # Fetch the metrics for the current run\n",
        "    metrics = run.summary._json_dict\n",
        "\n",
        "    # Check if the 'accuracy' metric is in the current run's metrics\n",
        "    if 'train/loss' in metrics:\n",
        "        # Compare the accuracy with the highest accuracy found so far\n",
        "        if metrics['train/loss'] < lowest_loss:\n",
        "            lowest_loss = metrics['train/loss']\n",
        "            best_run = run\n",
        "\n",
        "# Print the best run and its accuracy\n",
        "print(f\"The best run is {best_run.id} with an train loss of {lowest_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q4zQYOI-GQ0"
      },
      "source": [
        "## Adding Model Weights to W&B Registry\n",
        "- Here we get our best checkpoint from the finetuning run and register it as our best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-CPBS1X-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "registered_model_name = \"OPT-125M-english\"\n",
        "\n",
        "REGISTRY_NAME = \"model\"\n",
        "COLLECTION_TYPE = \"model\"\n",
        "\n",
        "run = wandb.init(project=PROJECT,\n",
        "                 entity=ENTITY,\n",
        "                 job_type=\"registering_best_model\")\n",
        "\n",
        "best_model = wandb.use_artifact(f'{ENTITY}/{PROJECT}/model-{best_run.id}:latest')\n",
        "\n",
        "\n",
        "run.link_artifact(best_model, f\"wandb_Y72QKAKNEFI3G/wandb-registry-model/{registered_model_name}\") #change registered model name to path supplied in UI\n",
        "\n",
        "\n",
        "run.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzDYSfMI-GQ0"
      },
      "source": [
        "## Consuming Model From Registry and Quantizing using ctranslate2\n",
        "- LLMs are typically too large to run in full-precision on even decent hardware.\n",
        "- You can quantize the model to run it more efficiently with minimal loss in accuracy.\n",
        "   - CTranslate2 is a great first pass at quantization but doesn't do \"smart\" quantization. It just converts all weights to half precision.\n",
        "   - Checkout out GPTQ and AutoGPTQ for SOTA quantization at scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyTUiG33-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Pull model from the registry\n",
        "\n",
        "wandb.init(project=PROJECT, entity=ENTITY, job_type=\"ctranslate2\")\n",
        "best_model = wandb.use_artifact(f'wandb_<>/wandb-registry-model/{registered_model_name}:latest') #change registrt path to path supplied in UI\n",
        "best_model.download(root=f'models/{registered_model_name}:latest')\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ5NlCXD-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "def convert_qlora2ct2(adapter_path=f'models/{registered_model_name}:latest',\n",
        "                      full_model_path=\"opt125m-finetuned\",\n",
        "                      offload_path=\"opt125m-offload\",\n",
        "                      ct2_path=\"opt125m-finetuned-ct2\",\n",
        "                      quantization=\"int8\"):\n",
        "\n",
        "\n",
        "    peft_model_id = adapter_path\n",
        "    peftconfig = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      \"facebook/opt-125m\",\n",
        "      offload_folder  = offload_path,\n",
        "      device_map='auto',\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
        "\n",
        "    model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "    print(\"Peft model loaded\")\n",
        "\n",
        "    merged_model = model.merge_and_unload()\n",
        "\n",
        "    merged_model.save_pretrained(full_model_path)\n",
        "    tokenizer.save_pretrained(full_model_path)\n",
        "\n",
        "    if quantization == False:\n",
        "        os.system(f\"ct2-transformers-converter --model {full_model_path} --output_dir {ct2_path} --force\")\n",
        "    else:\n",
        "        os.system(f\"ct2-transformers-converter --model {full_model_path} --output_dir {ct2_path} --quantization {quantization} --force\")\n",
        "    print(\"Convert successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7smQkXW-GQ0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "convert_qlora2ct2(adapter_path=f'models/{registered_model_name}:latest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRkLaSEF-GQ0"
      },
      "source": [
        "## Run Inference Using Quantized CTranslate2 Model\n",
        "- Record the results in a W&B Table!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nd8QCiv-GQ1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import ctranslate2\n",
        "\n",
        "\n",
        "run = wandb.init(project=PROJECT, entity=ENTITY, job_type=\"inference\")\n",
        "run.use_artifact(f'{ENTITY}/{PROJECT}/model-{best_run.id}:latest')\n",
        "\n",
        "generator = ctranslate2.Generator(\"opt125m-finetuned-ct2\")\n",
        "\n",
        "prompts = [\"Hey, are you conscious? Can you talk to me?\",\n",
        "           \"What is machine learning?\",\n",
        "           \"What is W&B?\"]\n",
        "\n",
        "\n",
        "wandb_table = wandb.Table(columns=['prompt', 'completion'])\n",
        "for prompt in prompts:\n",
        "  start_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt))\n",
        "  results = generator.generate_batch([start_tokens], max_length=30)\n",
        "  output = tokenizer.decode(results[0].sequences_ids[0])\n",
        "  wandb_table.add_data(prompt, output)\n",
        "\n",
        "wandb.log({\"inference_table\": wandb_table})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPcGtme6hH_k",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
