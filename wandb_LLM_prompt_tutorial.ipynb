{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Weight & Biases: Prompt Tracing and Evaluation with **Models** ðŸª„\n",
        "![](https://drive.google.com/uc?export=view&id=1OMfNfY2ApC575UtXhH5WZlNcdbMurT_m)  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This tutorial leverages 4 core components of W&B's Model suite:\n",
        "\n",
        "- **Run**: Experiment tracking context to monitor prompt iterations, configuration, execution context, and metrics.\n",
        "- **Artifact**: Version, governance and track lineage for data assets in a prompting pipeline.\n",
        "- **Table**: Interactive, tabular dataset to inspect generations.\n",
        "- **Sweep**: Hyperparameter tuning to optimize prompting and generation workflows."
      ],
      "metadata": {
        "id": "WufZOTIfbCO0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECWEbWxmOXIs"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8PudWpaD6hL"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq wandb openai tiktoken langchain-openai langchain transformers datasets evaluate rouge_score langchain-community chromadb asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd1sZ3TvJMtV"
      },
      "source": [
        "## Log in to W&B\n",
        "- You can explicitly login using `wandb login` or `wandb.login()` (See below)\n",
        "- Alternatively you can set environment variables. There are several env variables which you can set to change the behavior of W&B logging. The most important are:\n",
        "    - `WANDB_API_KEY` - find this in your \"Settings\" section under your profile\n",
        "    - `WANDB_BASE_URL` - this is the url of the W&B server\n",
        "- Find your API Token in \"Profile\" -> \"Setttings\" in the W&B App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkeNW9-NKT3-"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "from getpass import getpass\n",
        "import os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI, OpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-1LPVFKJmPx"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYX6pgxgJx4l"
      },
      "source": [
        "### Set OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJE3gv9iJxXb"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
        "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
        "\n",
        "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
        "print(\"OpenAI API key configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iljuvCEPPOW2"
      },
      "source": [
        "### Set Project and Entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x94kji2_PSLI"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = \"<>\" #@param\n",
        "ENTITY = \"<>\" #@param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shnxelFFOJ-K"
      },
      "source": [
        "## Download Dataset\n",
        "* Our Task is to summarize legal documents from the state of CA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyPhJXl-FbCh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"inspect_data\")\n",
        "\n",
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
        "\n",
        "# Let's just grab 5 documents for a demo\n",
        "shuffled_dataset = billsum.shuffle(seed=42)[0:5]\n",
        "\n",
        "billsum_df = pd.DataFrame.from_dict(shuffled_dataset)\n",
        "billsum_downsampled = wandb.Table(dataframe=billsum_df)\n",
        "\n",
        "#create an artifact from the dataset for version control + lineage tracking\n",
        "artifact = wandb.Artifact(\"ground_truth_data_ca\", type=\"datasets\")\n",
        "\n",
        "# Add the table to the artifact\n",
        "artifact.add(billsum_downsampled, 'downsampled_data')\n",
        "\n",
        "# Log the table + Artifact\n",
        "wandb.log({\"billsum_downsampled\": billsum_downsampled})\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5t84cJKm3Ac"
      },
      "source": [
        "## W&B Runs Track LLM Executions\n",
        "Below we have a simple Python function which invokes an LLM (OpenAI) to summarize documents contained in a pandas dataframe. Throughout this tutorial we will enrich this function with more logging to better understand our results and compare them across experiments:\n",
        "*   Use `wandb.init` to create a `Run`\n",
        "*   Use `wandb.config` to log \"inputs\" to your runs\n",
        "*   Use `wandb.Table` and `wandb.log` to log \"outputs\" or results of those runs\n",
        "\n",
        "By instrumenting logging in your LLM function calls, unit-testing, evaluation, and meta-analysis across models and parameters become much easier."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\""
      ],
      "metadata": {
        "id": "BE8dhGgIlAZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDQ_yzkbMRVx"
      },
      "outputs": [],
      "source": [
        "summarize_prompt1 = \"\"\"You have been provided with legal documents from the state of CA.\n",
        "    Your task is to provide a brief and comprehensive summary of the document.\n",
        "    Include 3 key points from the document:\n",
        "    {text}\n",
        "\n",
        "    SUMMARY:\"\"\"\n",
        "\n",
        "\n",
        "def llm_summarize_documents(df=billsum_df,\n",
        "                            model_name=\"gpt-4o-mini\",\n",
        "                            summarize_prompt=summarize_prompt1,\n",
        "                            temperature=0.1):\n",
        "\n",
        "\n",
        "  wandb.init(project=PROJECT_NAME,\n",
        "              entity=ENTITY,\n",
        "              job_type=\"summarize\",\n",
        "              config={\"model_name\": model_name,\n",
        "                      \"summarize_prompt\": summarize_prompt,\n",
        "                      \"temperature\": temperature})\n",
        "\n",
        "  wandb.use_artifact(f'{ENTITY}/{PROJECT_NAME}/ground_truth_data_ca:v0', type='datasets')\n",
        "\n",
        "  summarize_prompt_template = PromptTemplate(template=wandb.config[\"summarize_prompt\"], input_variables=[\"text\"])\n",
        "  llm = ChatOpenAI(model_name = wandb.config[\"model_name\"], temperature = wandb.config[\"temperature\"])\n",
        "\n",
        "  def llm_summarize_row(row):\n",
        "      with get_openai_callback() as cb:\n",
        "        # Prepare the prompt with the document text\n",
        "        document = row[\"text\"]\n",
        "        prompt = summarize_prompt_template.format(text=document)\n",
        "        messages = [\n",
        "          SystemMessage(content=prompt),\n",
        "        ]\n",
        "\n",
        "        # Get the summary from the LLM\n",
        "        summary = llm.invoke(messages).content\n",
        "\n",
        "        return {\"llm_summary\": summary,\n",
        "                \"prompt_tokens\": cb.prompt_tokens,\n",
        "                \"completion_tokens\": cb.completion_tokens,\n",
        "                \"total_tokens\": cb.total_tokens,\n",
        "                \"total_cost\": cb.total_cost}\n",
        "\n",
        "\n",
        "  df_llm = df.apply(llm_summarize_row, axis=1, result_type='expand')\n",
        "  df = df.join(df_llm)\n",
        "\n",
        "  # Log Pandas dataframes of results to interactive tables with built-in lineage and visualization\n",
        "  summary_table = wandb.Table(dataframe=df)\n",
        "\n",
        "  # also log as Artifact\n",
        "  artifact = wandb.Artifact(\"summary_df\", type=\"datasets\")\n",
        "  # Add the table to the artifact\n",
        "  artifact.add(summary_table, 'summary_table')\n",
        "\n",
        "  wandb.log({\"llm_inference/llm_summary_table\": summary_table})\n",
        "  wandb.log_artifact(artifact)\n",
        "\n",
        "\n",
        "  wandb.finish()\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MuriGL-pej9"
      },
      "outputs": [],
      "source": [
        "billsum_df_llm = llm_summarize_documents(billsum_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIt1PI7xqiTb"
      },
      "source": [
        "## Log Custom Metrics\n",
        "\n",
        "- `wandb.log` can be passed a dictionary of keys and values where the keys are the names of the metrics and the values are `wandb.Table`s, scalar metrics, numpy array embeddings, or even charts from matplotlib,\n",
        "- `wandb.summary` can be used to track metrics that describe the entire run, usually aggregate metrics like `total_cost` of all evaluations for instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWfbCgAXrFRA"
      },
      "outputs": [],
      "source": [
        "def llm_summarize_documents(df=billsum_df,\n",
        "                            model_name=\"gpt-4\",\n",
        "                            summarize_prompt=summarize_prompt1,\n",
        "                            temperature=0.5):\n",
        "\n",
        "  wandb.init(project=PROJECT_NAME,\n",
        "              entity=ENTITY,\n",
        "              job_type=\"summarize\",\n",
        "              config={\"model_name\": model_name,\n",
        "                      \"summarize_prompt\": summarize_prompt,\n",
        "                      \"temperature\": temperature})\n",
        "\n",
        "  wandb.use_artifact(f'{ENTITY}/{PROJECT_NAME}/ground_truth_data_ca:v0', type='datasets')\n",
        "\n",
        "  summarize_prompt_template = PromptTemplate(template=wandb.config[\"summarize_prompt\"], input_variables=[\"text\"])\n",
        "  llm = ChatOpenAI(model_name = wandb.config[\"model_name\"], temperature = wandb.config[\"temperature\"])\n",
        "\n",
        "  def llm_summarize_row(row):\n",
        "      with get_openai_callback() as cb:\n",
        "        # Prepare the prompt with the document text\n",
        "        document = row[\"text\"]\n",
        "        prompt = summarize_prompt_template.format(text=document)\n",
        "        messages = [\n",
        "          SystemMessage(content=prompt),\n",
        "        ]\n",
        "\n",
        "        # Get the summary from the LLM\n",
        "        summary = llm.invoke(prompt).content\n",
        "\n",
        "        return {\"llm_summary\": summary,\n",
        "                \"prompt_tokens\": cb.prompt_tokens,\n",
        "                \"completion_tokens\": cb.completion_tokens,\n",
        "                \"total_tokens\": cb.total_tokens,\n",
        "                \"total_cost\": cb.total_cost}\n",
        "\n",
        "\n",
        "  df_llm = df.apply(llm_summarize_row, axis=1, result_type='expand')\n",
        "  df = df.join(df_llm)\n",
        "\n",
        "\n",
        "  # Eval Metrics\n",
        "\n",
        "  # Add Rouge metric calculation\n",
        "  rouge = evaluate.load('rouge')\n",
        "  results = rouge.compute(predictions=df[\"llm_summary\"],\n",
        "                         references=df[\"summary\"],\n",
        "                        use_aggregator=False)\n",
        "\n",
        "  rouge_df = pd.DataFrame.from_dict(results)\n",
        "  df = df.join(rouge_df)\n",
        "\n",
        "  # toxicity measure\n",
        "  toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
        "  results_toxic = toxicity.compute(predictions=df['llm_summary'])\n",
        "  toxic_df = pd.DataFrame.from_dict(results_toxic)\n",
        "  df = df.join(toxic_df)\n",
        "\n",
        "  # Log Pandas dataframes of results to interactive tables with built-in lineage and visualization\n",
        "  df['llm_used'] = wandb.config.model_name\n",
        "  summary_table = wandb.Table(dataframe=df)\n",
        "\n",
        "  # also log as Artifact\n",
        "  artifact = wandb.Artifact(\"summary_df_metrics\", type=\"datasets\")\n",
        "  # Add the table to the artifact\n",
        "  artifact.add(summary_table, 'summary_table_metrics')\n",
        "\n",
        "  # Log additional metrics as part of wandb.log call\n",
        "  wandb.log({\"llm_summary_table\": summary_table})\n",
        "  wandb.log_artifact(artifact)\n",
        "\n",
        "  wandb.summary[\"rouge1\"] = df[\"rouge1\"].mean()\n",
        "  wandb.summary[\"rouge2\"] = df[\"rouge2\"].mean()\n",
        "  wandb.summary[\"rougeL\"] = df[\"rougeL\"].mean()\n",
        "  wandb.summary[\"rougeLsum\"] = df[\"rougeLsum\"].mean()\n",
        "  wandb.summary[\"total_cost\"] = df[\"total_cost\"].sum()\n",
        "\n",
        "  wandb.finish()\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObVWyIjKwkVA"
      },
      "outputs": [],
      "source": [
        "billsum_df_llm = llm_summarize_documents(billsum_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgIpe2sgsdRk"
      },
      "source": [
        "## Retrieving from W&B\n",
        "- After evaluating an LLM and prompts on a dataset, we want to then retrieve info from past runs which we like and instrument them in a pipeline\n",
        "- the `wandb.Api` import/export api allows you to retrieve runs, metrics, and tables via the api and hand-off evaluation results or prompts from one funciton to another"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viuIqJzrg-Cx"
      },
      "outputs": [],
      "source": [
        "past_run_id = \"<run id from above>\"\n",
        "\n",
        "def retrieve_wandb_table(project: str, entity: str, wandb_run_id: str, table_name: str, table_version: str) -> pd.DataFrame:\n",
        "  if wandb.run is None:\n",
        "    api = wandb.Api(overrides={\"project\": project,\n",
        "                              \"entity\": entity})\n",
        "    table_art = api.artifact(name=f\"run-{wandb_run_id}-{table_name}:{table_version}\")\n",
        "  else:\n",
        "    table_art = wandb.use_artifact(f\"run-{wandb_run_id}-{table_name}:{table_version}\")\n",
        "  table = table_art.get(table_name)\n",
        "  table_df = pd.DataFrame(data=table.data, columns=table.columns)\n",
        "  return table_df\n",
        "\n",
        "table_df = retrieve_wandb_table(PROJECT_NAME, ENTITY, past_run_id, \"llm_summary_table\" , \"latest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvI6LRbOKc06"
      },
      "outputs": [],
      "source": [
        "table_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAyJXOT2lqm6"
      },
      "source": [
        "### Retrieve run configs (e.g. prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsh69d2CvIZO"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api(overrides={\"project\": PROJECT_NAME,\n",
        "                              \"entity\": ENTITY})\n",
        "run = api.run(f\"{ENTITY}/{PROJECT_NAME}/{past_run_id}\")\n",
        "\n",
        "print(run.config['summarize_prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.config"
      ],
      "metadata": {
        "id": "-CNUvxEUj5Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maokl-z4criy"
      },
      "source": [
        "## LLM Evaluation\n",
        "* We can ask GPT4 to verify if the summaries are correct from a previous run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgH12CgActCX"
      },
      "outputs": [],
      "source": [
        "def gpt_evaluate_summaries(wandb_run_id: str,\n",
        "                            table_name: str,\n",
        "                            table_version: str):\n",
        "  wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"gpt_evaluation\")\n",
        "\n",
        "  eval_df = retrieve_wandb_table(PROJECT_NAME, ENTITY, wandb_run_id, table_name, table_version)\n",
        "  evaluate_prompt = \"\"\"Here are two summaries of some legal documents. \\\n",
        "  The first one is a human written summary and the second is generated by an LLM. \\\n",
        "  Please indicate whether the LLM-summary is accurate to the human one. Just give a YES or NO: \\n \\\n",
        "  HUMAN SUMMARY: {human_summary} \\n \\\n",
        "  LLM_SUMMARY: {llm_summary} \\n \\\n",
        "  ANSWER:\"\"\"\n",
        "\n",
        "  evaluate_prompt_template = PromptTemplate(template=evaluate_prompt, input_variables=[\"human_summary\", \"llm_summary\"])\n",
        "  llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "\n",
        "  def evaluate_summaries(row):\n",
        "    human_summary = row[\"summary\"]\n",
        "    llm_summary = row[\"llm_summary\"]\n",
        "    with get_openai_callback() as cb:\n",
        "        # Prepare the prompt with the document text\n",
        "        prompt = evaluate_prompt_template.format(human_summary=human_summary, llm_summary=llm_summary)\n",
        "        messages = [\n",
        "          SystemMessage(content=prompt),\n",
        "        ]\n",
        "        result = llm.invoke(messages).content\n",
        "\n",
        "        return {\"gpt_result\": result,\n",
        "                \"prompt_tokens\": cb.prompt_tokens,\n",
        "                \"completion_tokens\": cb.completion_tokens,\n",
        "                \"total_tokens\": cb.total_tokens,\n",
        "                \"total_cost\": cb.total_cost}\n",
        "\n",
        "  eval_result = eval_df.apply(evaluate_summaries, axis=1, result_type='expand')\n",
        "  eval_df = eval_df.join(eval_result, rsuffix=\"gpt4_eval_\")\n",
        "\n",
        "  eval_table = wandb.Table(dataframe=eval_df)\n",
        "\n",
        "  wandb.log({\"llm_eval/gpt_eval_table\": eval_table})\n",
        "  wandb.summary[\"total_cost\"] = eval_df[\"total_cost\"].sum()\n",
        "\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_5JFp2EqK58"
      },
      "outputs": [],
      "source": [
        "gpt_evaluate_summaries(past_run_id, \"llm_summary_table\", \"latest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trace Rag pipeline"
      ],
      "metadata": {
        "id": "k-3M6lXol1vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings(table_df: pd.DataFrame, index: int):\n",
        "\n",
        "  # load docs into langchain format\n",
        "  loader = DataFrameLoader(table_df, page_content_column=\"text\")\n",
        "  data = loader.load()\n",
        "\n",
        "  # split the documents\n",
        "  text_splitter = TokenTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)\n",
        "  docs = text_splitter.split_documents(data)\n",
        "\n",
        "  title = data[0].metadata[\"title\"]\n",
        "\n",
        "  # initialize embedding engine\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "\n",
        "  db = Chroma.from_documents(\n",
        "      docs,\n",
        "      embeddings,\n",
        "      persist_directory=os.path.join(\"chromadb\", str(index)),\n",
        "  )\n",
        "  db.persist()\n",
        "\n",
        "\n",
        "wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"create_embeddings\")\n",
        "\n",
        "# create embeddings\n",
        "with get_openai_callback() as cb:\n",
        "  for document in table_df.iterrows():\n",
        "    document_df = document[1].to_frame().T\n",
        "    create_embeddings(document_df, index=document[0])\n",
        "\n",
        "\n",
        "vector_db_artifact = wandb.Artifact(\"vector_db\", type=\"vector_db\")\n",
        "vector_db_artifact.add_dir(\"chromadb\")\n",
        "wandb.log_artifact(vector_db_artifact)\n",
        "\n",
        "wandb.summary[\"prompt_tokens\"] = cb.prompt_tokens\n",
        "wandb.summary[\"completion_tokens\"] = cb.completion_tokens\n",
        "wandb.summary[\"total_tokens\"] = cb.total_tokens\n",
        "wandb.summary[\"total_cost\"] = cb.total_cost\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "YDGh3wWXZ2aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(document_title: str, question: str):\n",
        "  index = table_df[table_df[\"title\"] == document_title].index[0]\n",
        "  db_dir = os.path.join(\"chromadb\", str(index))\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  db = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
        "\n",
        "  prompt_template = \"\"\"Use the following pieces of context to answer the question.\n",
        "  If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "  Don't add your opinions or interpretations. Ensure that you complete the answer.\n",
        "  If the question is not relevant to the context, just say that it is not relevant.\n",
        "\n",
        "  CONTEXT:\n",
        "  {context}\n",
        "\n",
        "  QUESTION: {question}\n",
        "\n",
        "  ANSWER:\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "  retriever = db.as_retriever()\n",
        "  retriever.search_kwargs[\"k\"] = 2\n",
        "\n",
        "  qa = RetrievalQA.from_chain_type(\n",
        "      llm=ChatOpenAI(temperature=0),\n",
        "      chain_type=\"stuff\",\n",
        "      retriever=retriever,\n",
        "      chain_type_kwargs={\"prompt\": prompt},\n",
        "      return_source_documents=True\n",
        "  )\n",
        "\n",
        "  with get_openai_callback() as cb:\n",
        "      result = qa({\"query\": question})\n",
        "\n",
        "  answer = result[\"result\"]\n",
        "  return answer"
      ],
      "metadata": {
        "id": "ML-D6-VTg-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = table_df[\"title\"][0]"
      ],
      "metadata": {
        "id": "auDd8fljg_8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"retrieval_QA\")\n",
        "wandb.use_artifact(f'{ENTITY}/{PROJECT_NAME}/vector_db:v0', type='vector_db')\n",
        "get_answer(title,\n",
        "            \"What does the law say about statewide emissions?\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "vTtmF-YNhBWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK21EudxcLIu"
      },
      "source": [
        "## Execute Sweeps Across Prompts and Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aAlJrk3HV8w"
      },
      "outputs": [],
      "source": [
        "summarize_prompt1 = \"\"\"You have been provided with legal documents from the state of CA.\n",
        "    Your task is to provide a brief and comprehensive summary of the document.\n",
        "    The summary should encompass all the crucial points of the document.\n",
        "    Include at least 3 points and ensure the summary is at least 2 paragraphs long\n",
        "    {text}\n",
        "\n",
        "    SUMMARY:\"\"\"\n",
        "\n",
        "summarize_prompt2 = \"\"\"You are an expert in State Law in CA. You have been provided with legal documents from the state of CA.\n",
        "    Your task is to provide a brief and comprehensive summary of the document.\n",
        "    The summary should encompass all the crucial points of the document and do not be vague.\n",
        "    {text}\n",
        "\n",
        "    SUMMARY:\"\"\"\n",
        "\n",
        "summarize_prompt3 = \"\"\"You are an expert in State Law in CA. You have been provided with legal documents from the state of CA.\n",
        "    Your task is to provide a brief and comprehensive summary of the document for the purposes of review by the state legal office.\n",
        "    The summary should encompass all the crucial points but do not be so vague so as to lose the ability to categorize the document effectively\n",
        "    from a legal standpoint:\n",
        "    {text}\n",
        "\n",
        "    SUMMARY:\"\"\"\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "}\n",
        "\n",
        "parameters_dict = {\n",
        "    'summarize_prompt': {\n",
        "        'values': [summarize_prompt1, summarize_prompt2, summarize_prompt3]\n",
        "    },\n",
        "    'model_name': {\n",
        "        'values': [\"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-4\"]\n",
        "    },\n",
        "    'temperature': {\n",
        "        'values': [0.1, 0.2, 0.3]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME, entity=ENTITY)\n",
        "# This sweep id you will pass to the agents later running on your machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar3UU-cGLRgE"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, llm_summarize_documents, count=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtegMK-dzMHP"
      },
      "outputs": [],
      "source": [
        "wandb.teardown()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}